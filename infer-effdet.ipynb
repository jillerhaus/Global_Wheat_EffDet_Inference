{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global-Wheat-Detection: EfficientDet Inference\n",
    "\n",
    "This is the inference notebook to my EfficientDet pipeline. It is based on Alex Shonenkov's [notebook](https://www.kaggle.com/shonenkov/inference-efficientdet), since before this I have never used EfficientDet and the documentation of the PyTorch implementation I am using is very spotty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Detection\n",
    "\n",
    "This is to make sure the notebook works on both my own windows workstation as well as on the Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "system = os.name\n",
    "if system == 'posix':\n",
    "    kaggle = True\n",
    "    windows = False\n",
    "    print('running on kaggle')\n",
    "elif system == 'nt':\n",
    "    kaggle = False\n",
    "    windows = True\n",
    "    print('running on windows')\n",
    "else:\n",
    "    print('unknown system')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    import re\n",
    "    import cv2\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    from effdet import get_efficientdet_config, EfficientDet, DetBenchPredict\n",
    "    from effdet.efficientdet import HeadNet\n",
    "    import gc\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ensemble_boxes import *\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle:\n",
    "    !pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n",
    "#     !pip install --no-deps '../input/effdetgithub/'\n",
    "    !pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle:\n",
    "    import sys\n",
    "    sys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\n",
    "    sys.path.insert(0, \"../input/omegaconf\")\n",
    "    sys.path.insert(0, \"../input/weightedboxesfusion\")\n",
    "\n",
    "    from ensemble_boxes import *\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    from torch.utils.data import Dataset,DataLoader\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch.transforms import ToTensorV2\n",
    "    import cv2\n",
    "    import gc\n",
    "    from matplotlib import pyplot as plt\n",
    "    from effdet import get_efficientdet_config, EfficientDet, DetBenchEval\n",
    "    from effdet.efficientdet import HeadNet\n",
    "    import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of all test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    path = f'../input/global-wheat-detection/train'\n",
    "#     path = f'../input/global-wheat-detection/tester'\n",
    "if kaggle:\n",
    "    path = f'../input/global-wheat-detection/test'\n",
    "jpg_paths = glob(f'{path}/*.jpg')\n",
    "jpgfilenames = [re.compile(r'([\\w\\_\\(\\)]*)\\.jpg').search(jpgpath).group(1) for jpgpath in jpg_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configuration():\n",
    "    scale = 1\n",
    "    batch_size = 8\n",
    "    if windows:\n",
    "        cuda_device = 0\n",
    "    if kaggle:\n",
    "        cuda_device = 0\n",
    "    if kaggle:\n",
    "        gt = False\n",
    "        visualize = False\n",
    "        save = False\n",
    "    if windows:\n",
    "        gt = True\n",
    "        visualize = True\n",
    "        save = True\n",
    "    picture_dims = [1024, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever for testing data\n",
    "pic_scale = {}\n",
    "class DatasetRetriever():\n",
    "    \n",
    "    def __init__(self, image_ids, transforms = None):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.scale = configuration.scale\n",
    "        self.picture_dims = configuration.picture_dims\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self, index: int):\n",
    "        if self.transforms == None:\n",
    "            return self.load_item(index)\n",
    "        else:\n",
    "            return(self.transform_val(self.load_item(index),index, self.transforms))\n",
    "    \n",
    "    def load_item(self, index:int):\n",
    "        filename = self.image_ids[index]\n",
    "        image = f'{path}/{filename}.jpg'\n",
    "        image = cv2.imread(image, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        pic_scale[filename] = image.shape\n",
    "        image /= 255.0\n",
    "        #resize the image\n",
    "        w_image = int(image.shape[1] * self.scale)\n",
    "        h_image = int(image.shape[0] * self.scale)\n",
    "#         dim = (w_image, h_image)\n",
    "        dim = (self.picture_dims[0],self.picture_dims[1])\n",
    "        image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "        image = torch.tensor(image).permute(2,0,1)\n",
    "        \n",
    "        return image, filename\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def transform_val(self, image_data, index, transforms):\n",
    "        image, filename = image_data\n",
    "    \n",
    "        if transforms == 'all':\n",
    "            transforms =transforms = A.Compose(\n",
    "            [\n",
    "                A.RandomSizedCrop(min_max_height=(400, 400), height=512, width=512, p=0.5),\n",
    "                A.OneOf([\n",
    "                    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                         val_shift_limit=0.2, p=0.9),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                               contrast_limit=0.2, p=0.9),\n",
    "                ],p=0.9),\n",
    "                A.ToGray(p=0.01),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.Resize(height=512, width=512, p=1),\n",
    "                A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], \n",
    "            p=1.0, \n",
    "    #         bbox_params=A.BboxParams(\n",
    "    #             format='pascal_voc',\n",
    "    #             min_area=0, \n",
    "    #             min_visibility=0,\n",
    "    #             label_fields=['labels']\n",
    "    #         )\n",
    "            )\n",
    "        elif transforms == 'bnw':\n",
    "                transforms = transforms = A.Compose([\n",
    "                    A.ToGray(p=1.0),\n",
    "                    ToTensorV2(p=1.0)\n",
    "                    \n",
    "                ], p = 1.0)\n",
    "                \n",
    "        elif transforms == 'solar':\n",
    "                transforms = transforms = A.Compose([\n",
    "                    A.RGBShift(r_shift_limit=(0.1,0.1), g_shift_limit=(0.2,0.2), b_shift_limit=(0.0,0.0), always_apply=True),\n",
    "                    ToTensorV2(p=1.0)\n",
    "                    \n",
    "                ], p = 1.0)\n",
    "                \n",
    "        elif transforms == 'bright':\n",
    "               transforms = transforms = A.Compose([\n",
    "                   A.RandomBrightness(limit=0.2, p=1),\n",
    "                   ToTensorV2(p=1.0)\n",
    "                   \n",
    "               ], p = 1.0)\n",
    "                \n",
    "        elif transforms == 'vertflip':\n",
    "               transforms = transforms = A.Compose([\n",
    "                   A.HueSaturationValue(hue_shift_limit=(0.0,0.0), sat_shift_limit= (0.3,0.3), \n",
    "                                         val_shift_limit=(-0.1,-0.1), p=1.0),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=(-0.05,-0.05), \n",
    "                                               contrast_limit=(0.3,0.3), p=1.0),\n",
    "                    A.RandomGamma(gamma_limit=(75, 75), eps=None, p=1.0),\n",
    "                   A.VerticalFlip(p=1.0),\n",
    "                   ToTensorV2(p=1.0)\n",
    "                   \n",
    "               ], p = 1.0)\n",
    "                \n",
    "        elif transforms == 'horflip':\n",
    "               transforms = transforms = A.Compose([\n",
    "                   A.HueSaturationValue(hue_shift_limit=(0.0,0.0), sat_shift_limit= (0.3,0.3), \n",
    "                                         val_shift_limit=(-0.1,-0.1), p=1.0),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=(-0.05,-0.05), \n",
    "                                               contrast_limit=(0.3,0.3), p=1.0),\n",
    "                    A.RandomGamma(gamma_limit=(75, 75), eps=None, p=1.0),\n",
    "                   A.HorizontalFlip(p=1.0),\n",
    "                   ToTensorV2(p=1.0)\n",
    "                   \n",
    "               ], p = 1.0)\n",
    "                \n",
    "        elif transforms == 'contrast':\n",
    "               transforms = A.Compose([\n",
    "                    A.HueSaturationValue(hue_shift_limit=(0.0,0.0), sat_shift_limit= (0.3,0.3), \n",
    "                                         val_shift_limit=(-0.1,-0.1), p=1.0),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=(-0.05,-0.05), \n",
    "                                               contrast_limit=(0.3,0.3), p=1.0),\n",
    "                    A.RandomGamma(gamma_limit=(75, 75), eps=None, p=1.0),\n",
    "                   ToTensorV2(p=1.0)\n",
    "                   \n",
    "               ], p = 1.0)\n",
    "#         elif transforms == 'rot_r':\n",
    "#                transforms = transforms = A.Compose([\n",
    "#                    A.RandomRotate90(p=0.5, apply(factor = 3)),\n",
    "#                    ToTensorV2(p=1.0)\n",
    "                   \n",
    "#                ], p = 1.0)        \n",
    "                \n",
    "\n",
    "        sample = transforms(**{'image': image.permute(1,2,0).cpu().numpy()#,\n",
    "#               'bboxes': target['boxes'][:,[1,0,3,2]].cpu().numpy(),\n",
    "#               'labels': target['labels'].cpu().numpy()\n",
    "             })\n",
    "        \n",
    "        for i in range(10):\n",
    "              if 1 == 1:\n",
    "#             if len(sample['bboxes']) > 0:\n",
    "                image = sample['image']\n",
    "#                 target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "#                 target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]] #yxyx: be warning\n",
    "#                 target['labels'] = torch.tensor(sample['labels'])\n",
    "                break\n",
    "        \n",
    "        output = (image, filename)\n",
    "        if len(output) == 2:\n",
    "            return output\n",
    "        else:\n",
    "            return self.load_item(index)\n",
    "dataset = DatasetRetriever(image_ids = jpgfilenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deaugment(image, boxes, labels, transform):\n",
    "    if transform in [None, 'bnw', 'solar', 'bright','contrast']:\n",
    "        return image, boxes\n",
    "    else:\n",
    "        \n",
    "        if transform == 'vertflip':\n",
    "            transforms = A.Compose(\n",
    "                [A.VerticalFlip(p=1.0),\n",
    "                 ToTensorV2(p=1.0)],\n",
    "                p = 1.0,\n",
    "            bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']))\n",
    "        \n",
    "        elif transform == 'horflip':\n",
    "            transforms = A.Compose(\n",
    "                [A.HorizontalFlip(p=1.0),\n",
    "                 ToTensorV2(p=1.0)],\n",
    "                p = 1.0,\n",
    "            bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']))\n",
    "            \n",
    "            \n",
    "        boxes[:,2] = boxes[:,2] + boxes[:,0]\n",
    "        boxes[:,3] = boxes[:,3] + boxes[:,1]\n",
    "        boxes = np.abs(np.array([boxes]))[0]\n",
    "        boxes[boxes < 0] = 0\n",
    "        boxes[boxes >= 1024] = 1023\n",
    "        \n",
    "        sample = transforms(**{'image': image.permute(1,2,0).cpu().numpy(),\n",
    "                              'bboxes': boxes,\n",
    "                               'labels': np.ones(boxes.shape[0])\n",
    "                              })\n",
    "        boxes = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "        labels = labels\n",
    "        boxes[:,2] = boxes[:,2] - boxes[:,0]\n",
    "        boxes[:,3] = boxes[:,3] - boxes[:,1]\n",
    "        return image, boxes\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if configuration.gt == True:\n",
    "    nr_files = len(jpgfilenames)\n",
    "    class DatasetRetriever1():\n",
    "        def __init__(self, df, image_ids):\n",
    "            super().__init__()\n",
    "            self.image_ids = image_ids\n",
    "            self.df = df\n",
    "            self.picture_dims = configuration.picture_dims\n",
    "        def __getitem__(self, index:int):\n",
    "            #load the image\n",
    "            filename = self.image_ids[index][:-2]\n",
    "            flavor = self.image_ids[index][-2:]\n",
    "            filename = self.image_ids[index]\n",
    "            image = f'{path}/{filename}.jpg'\n",
    "            image = cv2.imread(image, cv2.IMREAD_COLOR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "            \n",
    "            image /= 255.0\n",
    "            #resize the image\n",
    "            dim = (self.picture_dims[0],self.picture_dims[1])\n",
    "            image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "            image = torch.tensor(image).permute(2,0,1)\n",
    "            if filename in labels.image_id.values:\n",
    "                boxes = labels.loc[labels.image_id == filename,['x_min', 'y_min', 'x_max', 'y_max']]\n",
    "                boxes = torch.tensor(boxes.values)\n",
    "                boxes = boxes[:,[1,0,3,2]]\n",
    "                boxes = boxes.int()\n",
    "                lbls = torch.tensor(np.ones(boxes.shape[0], dtype = 'int32'))\n",
    "            else:\n",
    "                boxes = torch.tensor([])\n",
    "                lbls = torch.tensor([])\n",
    "            return image, dict(boxes = boxes, labels = lbls, image_id = torch.tensor([index])), filename\n",
    "        def __len__(self):\n",
    "            return len(self.image_ids)\n",
    "\n",
    "    labels = pd.read_csv('labels.csv')\n",
    "    dataset1 = DatasetRetriever1(df = labels, image_ids = jpgfilenames)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=configuration.batch_size,\n",
    "                                          pin_memory = False,\n",
    "                                          drop_last = False,\n",
    "                                          shuffle = False,\n",
    "                                          collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    net = EfficientDet(config = config, pretrained_backbone = False)\n",
    "    config.num_classes = 1\n",
    "    config.image_size = 1024 * configuration.scale\n",
    "    net.class_net = HeadNet(config, num_outputs = config.num_classes,\n",
    "                            norm_kwargs = dict(eps = .001, momentum = .01))\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    \n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "    if windows:\n",
    "        net = DetBenchPredict(net, config)\n",
    "    if kaggle:\n",
    "        net = DetBenchEval(net, config)\n",
    "    return net.cuda(configuration.cuda_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "#     chkpntpth = sorted(glob(f'../../train/working/effdet-experimental/best-checkpoint-*epoch.bin'))[-1]\n",
    "    chkpntpth =  f'../../train/working/effdet-experimental/best-checkpoint-026epoch.bin'\n",
    "    chkpntpth2 = f'../../train/working/effdet-experimental/last-checkpoint.bin'\n",
    "    chkpntpth3 = f'../../train/working/effdet-experimental/last-checkpoint-kaggle.bin'\n",
    "    chkpntpth4 = f'../../train/working/effdet-experimental/best-checkpoint-030epoch.bin'\n",
    "if kaggle:\n",
    "    chkpntpth = f'../input/effdetweights/best-checkpoint-026epoch.bin'\n",
    "    chkpntpth2 = f'../input/effdetweights/last-checkpoint.bin'\n",
    "    chkpntpth3 = f'../input/effdetweights/last-checkpoint-kaggle.bin'\n",
    "    chkpntpth4 = f'../input/effdetweights/best-checkpoint-030epoch.bin'\n",
    "    \n",
    "net = load_net(chkpntpth)\n",
    "net2 = load_net(chkpntpth2)\n",
    "net3 = load_net(chkpntpth3)\n",
    "net4 = load_net(chkpntpth4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(images, image_ids, model, transforms, score_threshold_orig):\n",
    "    prediction_list = []\n",
    "    images = torch.stack(images).cuda(configuration.cuda_device).float()\n",
    "    #, 'bad_conf':[], 'badboxes':[]}\n",
    "    \n",
    "    batch_size = images.shape[0]\n",
    "    images = images.cuda(configuration.cuda_device).float()\n",
    "    target_res = {}\n",
    "    target_res['img_scale'] = torch.tensor([1.0] * batch_size, dtype = torch.float32).cuda(configuration.cuda_device)\n",
    "    target_res['img_size'] = torch.tensor([images[0].shape[-2:]] * batch_size, dtype=torch.float32).cuda(configuration.cuda_device)\n",
    "    with torch.no_grad():\n",
    "        if windows:\n",
    "            det = model(images, target_res['img_scale'], target_res['img_size'])\n",
    "        if kaggle:\n",
    "            det = model(images, torch.tensor([1]*images.shape[0]).float().cuda())\n",
    "            \n",
    "        for i in range(images.shape[0]):\n",
    "            predictions = {'image_id':[],'conf': [],'boxes':[]}\n",
    "            # the first four columns are the boundaries of the boxes and the last 2 are the \n",
    "            score_threshold = score_threshold_orig\n",
    "            boxes = det[i].detach().cpu().numpy()[:,:4]\n",
    "            nonull = boxes[:,2] > 10\n",
    "            boxes = boxes[nonull]\n",
    "            boxes = deaugment(images[i],boxes,np.ones(boxes.shape[0]), transforms)[1]\n",
    "            scores = det[i].detach().cpu().numpy()[:,4]\n",
    "            scores = scores[nonull]\n",
    "            if boxes.shape[0] < 5:\n",
    "                score_threshold = 0.6\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            boxes = boxes[indexes]\n",
    "            scores = scores[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            #predictions.append({'boxes': boxes[indexes],\n",
    "             #                 'scores': scores[indexes]})\n",
    "            predictions['boxes'] = boxes\n",
    "            predictions['conf'] = scores\n",
    "            predictions['image_id'] = image_ids[i]\n",
    "#             predictions['bad_conf'].append()\n",
    "            prediction_list.append(predictions)\n",
    "    return prediction_list\n",
    "#     with torch.no_grad():\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wbs(images, image_ids,models, transform_list,\n",
    "              score_threshold_orig = 0.2, skip_box_thr = 0.001):\n",
    "    prediction_list = []\n",
    "    # make image_index_list\n",
    "    index_list = []\n",
    "    for image_id in image_ids:\n",
    "#         image_index = DatasetRetriever(image_ids = jpgfilenames, transforms = None).__getitem__(3)[1]\n",
    "        image_index = jpgfilenames.index(image_id)\n",
    "        index_list.append(image_index)\n",
    "    for model in models:\n",
    "        for transform in transform_list:\n",
    "            image_list = []\n",
    "            for index in index_list:\n",
    "                image = DatasetRetriever(image_ids = jpgfilenames,\n",
    "                                         transforms = transform).__getitem__(index)\n",
    "                image_list.append(image[0])\n",
    "            images = image_list\n",
    "            prediction_list.append(make_predictions(images, image_ids,\n",
    "                                                    score_threshold_orig = score_threshold_orig,\n",
    "                                                    model = model,transforms = transform))\n",
    "    predicts = []\n",
    "    prediction_df = pd.DataFrame(columns = ['image_id','conf','boxes'])\n",
    "    for prediction in prediction_list:\n",
    "        prediction_df = pd.concat([prediction_df,pd.DataFrame(prediction)], axis = 0)\n",
    "    nr_rows = prediction_df.shape[0]\n",
    "    pic_size = configuration.picture_dims[0]\n",
    "    prediction_df['normed_boxes'] = prediction_df['boxes'] / pic_size\n",
    "    for unique_id in prediction_df.image_id.unique():\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        boxes_list = []\n",
    "        scores_list = []\n",
    "        labels_list = []\n",
    "        small_df = prediction_df[prediction_df.image_id == unique_id].copy()\n",
    "        for i in range(small_df.shape[0]):\n",
    "            boxes_list.append(small_df.iloc[i].normed_boxes)\n",
    "            scores_list.append(small_df.iloc[i].conf)\n",
    "            labels_list.append([1]*small_df.iloc[i].conf.shape[0])\n",
    "        weights = [1]*len(models)*len(transform_list)\n",
    "        iou_thr = 0.45\n",
    "#         print(boxes_list)\n",
    "#         max_len = 0\n",
    "#         for array in scores_list:\n",
    "#             if len(array) > max_len:\n",
    "#                 max_len = len(array)\n",
    "#             if (max_len == 0):\n",
    "#                 predicts.append({'image_id' : unique_id, 'conf' : scores_list[0].astype(np.float32), 'boxes' : boxes_list[0].astype(np.float32)})\n",
    "        \n",
    "        if 1==0:\n",
    "            print('nothing')\n",
    "\n",
    "\n",
    "        else:\n",
    "            if len(models) == 1:\n",
    "                print('only one model')\n",
    "                boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list,\n",
    "                                                          weights = None, iou_thr = iou_thr,\n",
    "                                                         skip_box_thr = skip_box_thr)\n",
    "            else:\n",
    "                boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list,\n",
    "                                                          weights = weights, iou_thr = iou_thr,\n",
    "                                                         skip_box_thr=skip_box_thr,conf_type='avg')\n",
    "\n",
    "        thresh = 0.37\n",
    "        \n",
    "#         print(thresh)\n",
    "    \n",
    "        #             print(thresh)\n",
    "        indexes = np.where(scores > thresh)[0]\n",
    "        \n",
    "        boxes1 = boxes[indexes]\n",
    "        scores1 = scores[indexes]\n",
    "        if boxes.shape[0] < 5:\n",
    "            thresh = 0.5\n",
    "            indexes1 = np.where(scores > thresh)[0]\n",
    "            boxes1 = boxes1[indexes1]\n",
    "            scores1 = scores1[indexes1]\n",
    "        #             print(np.array(boxes,dtype = int))\n",
    "        \n",
    "        prediction_dict = {'image_id' : unique_id, 'conf': scores1.astype(np.float32), 'boxes': boxes1.astype(np.float32)*pic_size}\n",
    "        predicts.append(prediction_dict)\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_list = [None,'vertflip','horflip']#'solar','bright','horflip','bnw']\n",
    "transform_list = ['contrast','vertflip','horflip']\n",
    "model_list = [net2,net3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if configuration.visualize == True:\n",
    "    \n",
    "    dataset2 = DatasetRetriever(image_ids = jpgfilenames, transforms = None)\n",
    "    data_loader2 = torch.utils.data.DataLoader(dataset2,\n",
    "                                          batch_size=configuration.batch_size,\n",
    "                                          pin_memory = False,\n",
    "                                          drop_last = False,\n",
    "                                          shuffle = False,\n",
    "                                          collate_fn = collate_fn)\n",
    "    \n",
    "    m = 0\n",
    "    new_imagelist = []\n",
    "    inf_folder = f'./inference'\n",
    "    if configuration.save & (not os.path.exists(inf_folder)):\n",
    "                os.makedirs(inf_folder)\n",
    "    for j, (images, image_ids) in enumerate(data_loader2):\n",
    "        predictions = apply_wbs(images,image_ids, models = model_list,\n",
    "                                transform_list = transform_list) \n",
    "#         predictions = make_predictions(images, image_ids)\n",
    "        df = pd.DataFrame(predictions)\n",
    "        for i in range(len(image_ids)):\n",
    "            ## plotting the inference\n",
    "            image_id = image_ids[i]\n",
    "            new_imagelist.append(image_id)\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(16, 16))\n",
    "            bx1 = df.iloc[i].boxes\n",
    "            conf1 = df.iloc[i].conf\n",
    "            sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "            k = 0\n",
    "            for box in bx1:\n",
    "                cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 1)\n",
    "                plt.text(box[2],box[1],f'{conf1[k]:.3f}', color = 'red', fontsize = 18)\n",
    "                k+=1\n",
    "            #Plotting the Ground Truth\n",
    "            if configuration.gt == True:\n",
    "                image1, target1, image_id1 = dataset1[m]\n",
    "                boxes1 = target1['boxes'].cpu().numpy().astype(np.int32)\n",
    "                for box1 in boxes1:\n",
    "                    cv2.rectangle(sample, (box1[1], box1[0]), (box1[3],  box1[2]), (0, 1, 0), 2)\n",
    "\n",
    "            #Showing and saving the picture\n",
    "            ax.set_axis_off()\n",
    "            ax.imshow(sample)\n",
    "            if windows & configuration.save:\n",
    "                plt.savefig(f'{inf_folder}/{image_ids[i]}.jpeg', transparent=True, bbox_inches = 'tight',\n",
    "                            facecolor = 'k',pad_inches = 0)\n",
    "            plt.show()\n",
    "            m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns = ['image_id', 'PredictionString'])\n",
    "cols = ['conf', 'px_min', 'py_min', 'px_max', 'py_max']\n",
    "\n",
    "for j, (images, image_ids) in enumerate(data_loader):\n",
    "    predictions = apply_wbs(images, image_ids, models = model_list,transform_list = [None,'vertflip','solar','bright','horflip']) \n",
    "    df = pd.DataFrame(predictions)\n",
    "    for i in range(len(image_ids)):\n",
    "        row = df.iloc[i]\n",
    "        filename = row.image_id\n",
    "        dataset = pd.DataFrame(columns = cols)\n",
    "        if len(row.conf) > 0:\n",
    "            for k in range(len(row.conf)):\n",
    "                row_df = pd.DataFrame([[row.conf[k],row.boxes[k][0],row.boxes[k][1],row.boxes[k][2],row.boxes[k][3]]], \n",
    "                                      columns = cols)\n",
    "                dataset = pd.concat([dataset, row_df], axis = 0, ignore_index = True)\n",
    "            dataset['width'] = configuration.picture_dims[0]\n",
    "            dataset['height'] = configuration.picture_dims[1]\n",
    "            scale_x = pic_scale[filename][1]/1024\n",
    "            scale_y = pic_scale[filename][0]/1024\n",
    "            dataset['px_box'] = dataset['px_max'] - dataset['px_min']\n",
    "            dataset['py_box'] = dataset['py_max'] - dataset['py_min']\n",
    "            dataset[['px_min', 'py_min', 'px_box', 'py_box']] = dataset[['px_min', 'py_min', 'px_box', 'py_box']].round().astype('int')\n",
    "            dataset.loc[dataset.width < dataset.px_min,'px_min'] = dataset.width -1\n",
    "            dataset.loc[dataset.height < dataset.py_min,'py_min'] = dataset.height -1\n",
    "            dataset.loc[1 > dataset.px_box,'px_box'] = 1\n",
    "            dataset.loc[1 > dataset.py_box,'py_box'] = 1\n",
    "            dataset.loc[0 > dataset.px_min,'px_min'] = 0\n",
    "            dataset.loc[0 > dataset.py_min,'py_min'] = 0\n",
    "            dataset.loc[dataset['px_min'] + dataset['px_box'] > dataset['width'], 'px_box'] = dataset['width'] - dataset['px_min'] - 1\n",
    "            dataset.loc[dataset['py_min'] + dataset['py_box'] > dataset['height'], 'py_box'] = dataset['height'] - dataset['py_min'] - 1\n",
    "            dataset[['px_min','px_box']] = (dataset[['px_min','px_box']] * scale_x).astype(int)\n",
    "            dataset[['py_min','py_box']] = (dataset[['py_min','py_box']] * scale_y).astype(int)\n",
    "            submission_cols = ['conf', 'px_min', 'py_min', 'px_box', 'py_box']\n",
    "            shape = dataset[submission_cols].shape\n",
    "            row1 = dataset[submission_cols].values.reshape(1,shape[0]*shape[1])\n",
    "            str_array = np.array2string(row1[0], formatter = {'float_kind': lambda x: '%.5f' % x}, max_line_width = 9e3)\n",
    "            str_array = re.compile(r'\\[([\\d\\s\\.]+)\\]').search(str_array).group(1)\n",
    "            vals = pd.DataFrame([[filename, str_array]], columns =['image_id', 'PredictionString'])\n",
    "        else:\n",
    "            vals = pd.DataFrame([[filename, '']], columns =['image_id', 'PredictionString'])\n",
    "        submission = pd.concat([submission,vals], axis = 0, ignore_index = True)\n",
    "        submission.PredictionString = submission.PredictionString.str.replace(r'(\\.0+)', '').str.replace('\\s+',' ')\n",
    "\n",
    "submission.to_csv('submission.csv', index = False, sep = ',')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
