{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global-Wheat-Detection: EfficientDet Inference\n",
    "\n",
    "This is the inference notebook to my EfficientDet pipeline. It is based on Alex Shonenkov's [notebook](https://www.kaggle.com/shonenkov/inference-efficientdet), since before this I have never used EfficientDet and the documentation of the PyTorch implementation I am using is very spotty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Detection\n",
    "\n",
    "This is to make sure the notebook works on both my own windows workstation as well as on the Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "system = os.name\n",
    "if system == 'posix':\n",
    "    kaggle = True\n",
    "    windows = False\n",
    "    print('running on kaggle')\n",
    "elif system == 'nt':\n",
    "    kaggle = False\n",
    "    windows = True\n",
    "    print('running on windows')\n",
    "else:\n",
    "    print('unknown system')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    import re\n",
    "    import cv2\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    from effdet import get_efficientdet_config, EfficientDet, DetBenchPredict\n",
    "    from effdet.efficientdet import HeadNet\n",
    "    import gc\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ensemble_boxes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle:\n",
    "    !pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n",
    "#     !pip install --no-deps '../input/effdetgithub/'\n",
    "    !pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle:\n",
    "    import sys\n",
    "    sys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\n",
    "    sys.path.insert(0, \"../input/omegaconf\")\n",
    "    sys.path.insert(0, \"../input/weightedboxesfusion\")\n",
    "\n",
    "    from ensemble_boxes import *\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    from torch.utils.data import Dataset,DataLoader\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch.transforms import ToTensorV2\n",
    "    import cv2\n",
    "    import gc\n",
    "    from matplotlib import pyplot as plt\n",
    "    from effdet import get_efficientdet_config, EfficientDet, DetBenchEval\n",
    "    from effdet.efficientdet import HeadNet\n",
    "    import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of all test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "    path = f'../input/global-wheat-detection/train'\n",
    "#     path = f'../input/global-wheat-detection/tester'\n",
    "if kaggle:\n",
    "    path = f'../input/global-wheat-detection/test'\n",
    "jpg_paths = glob(f'{path}/*.jpg')\n",
    "jpgfilenames = [re.compile(r'([\\w\\_\\(\\)]*)\\.jpg').search(jpgpath).group(1) for jpgpath in jpg_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configuration():\n",
    "    scale = 1\n",
    "    batch_size = 2\n",
    "    if windows:\n",
    "        cuda_device = 1\n",
    "    if kaggle:\n",
    "        cuda_device = 0\n",
    "    if kaggle:\n",
    "        gt = False\n",
    "        visualize = False\n",
    "        save = False\n",
    "    if windows:\n",
    "        gt = True\n",
    "        visualize = True\n",
    "        save = True\n",
    "    picture_dims = [1024, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever for testing data\n",
    "pic_scale = {}\n",
    "class DatasetRetriever():\n",
    "    \n",
    "    def __init__(self, image_ids):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.scale = configuration.scale\n",
    "        self.picture_dims = configuration.picture_dims\n",
    "    def __getitem__(self, index: int):\n",
    "        filename = self.image_ids[index]\n",
    "        image = f'{path}/{filename}.jpg'\n",
    "        image = cv2.imread(image, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        pic_scale[filename] = image.shape\n",
    "        image /= 255.0\n",
    "        #resize the image\n",
    "        w_image = int(image.shape[1] * self.scale)\n",
    "        h_image = int(image.shape[0] * self.scale)\n",
    "#         dim = (w_image, h_image)\n",
    "        dim = (self.picture_dims[0],self.picture_dims[1])\n",
    "        image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "        image = torch.tensor(image).permute(2,0,1)\n",
    "        \n",
    "        return image, filename\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "dataset = DatasetRetriever(image_ids = jpgfilenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if configuration.gt == True:\n",
    "    nr_files = len(jpgfilenames)\n",
    "    class DatasetRetriever1():\n",
    "        def __init__(self, df, image_ids):\n",
    "            super().__init__()\n",
    "            self.image_ids = image_ids\n",
    "            self.df = df\n",
    "            self.picture_dims = configuration.picture_dims\n",
    "        def __getitem__(self, index:int):\n",
    "            #load the image\n",
    "            filename = self.image_ids[index][:-2]\n",
    "            flavor = self.image_ids[index][-2:]\n",
    "            filename = self.image_ids[index]\n",
    "            image = f'{path}/{filename}.jpg'\n",
    "            image = cv2.imread(image, cv2.IMREAD_COLOR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "            \n",
    "            image /= 255.0\n",
    "            #resize the image\n",
    "            dim = (self.picture_dims[0],self.picture_dims[1])\n",
    "            image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "            image = torch.tensor(image).permute(2,0,1)\n",
    "            if filename in labels.image_id.values:\n",
    "                boxes = labels.loc[labels.image_id == filename,['x_min', 'y_min', 'x_max', 'y_max']]\n",
    "                boxes = torch.tensor(boxes.values)\n",
    "                boxes = boxes[:,[1,0,3,2]]\n",
    "                boxes = boxes.int()\n",
    "                lbls = torch.tensor(np.ones(boxes.shape[0], dtype = 'int32'))\n",
    "            else:\n",
    "                boxes = torch.tensor([])\n",
    "                lbls = torch.tensor([])\n",
    "            return image, dict(boxes = boxes, labels = lbls, image_id = torch.tensor([index])), filename\n",
    "        def __len__(self):\n",
    "            return len(self.image_ids)\n",
    "\n",
    "    labels = pd.read_csv('labels.csv')\n",
    "    dataset1 = DatasetRetriever1(df = labels, image_ids = jpgfilenames)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=configuration.batch_size,\n",
    "                                          pin_memory = False,\n",
    "                                          drop_last = False,\n",
    "                                          shuffle = False,\n",
    "                                          collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    net = EfficientDet(config = config, pretrained_backbone = False)\n",
    "    config.num_classes = 1\n",
    "    config.image_size = 1024 * configuration.scale\n",
    "    net.class_net = HeadNet(config, num_outputs = config.num_classes,\n",
    "                            norm_kwargs = dict(eps = .001, momentum = .01))\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    \n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "    if windows:\n",
    "        net = DetBenchPredict(net, config)\n",
    "    if kaggle:\n",
    "        net = DetBenchEval(net, config)\n",
    "    return net.cuda(configuration.cuda_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if windows:\n",
    "#     chkpntpth = sorted(glob(f'../../train/working/effdet-experimental/best-checkpoint-*epoch.bin'))[-1]\n",
    "    chkpntpth2 = f'../../train/working/effdet-experimental/last-checkpoint.bin'\n",
    "#     chkpntpth3 = f'../../train/working/effdet-experimental/fold0-best-all-states.bin'\n",
    "if kaggle:\n",
    "    chkpntpth = f'../input/effdetweights/best-checkpoint-030epoch.bin'\n",
    "#     chkpntpth2 = f'../input/effdetweights/best-checkpoint-030epoch.bin'\n",
    "    chkpntpth3 = f'../input/effdetweights/last-checkpoint.bin'\n",
    "    \n",
    "# net = load_net(chkpntpth)\n",
    "net2 = load_net(chkpntpth2)\n",
    "# net3 = load_net(chkpntpth3)\n",
    "model_list = [net2]#, net2, net3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(images, image_ids, model, score_threshold_orig = 0.22):\n",
    "    prediction_list = []\n",
    "    images = torch.stack(images).cuda(configuration.cuda_device).float()\n",
    "    #, 'bad_conf':[], 'badboxes':[]}\n",
    "    \n",
    "    batch_size = images.shape[0]\n",
    "    images = images.cuda(configuration.cuda_device).float()\n",
    "    target_res = {}\n",
    "    target_res['img_scale'] = torch.tensor([1.0] * batch_size, dtype = torch.float32).cuda(configuration.cuda_device)\n",
    "    target_res['img_size'] = torch.tensor([images[0].shape[-2:]] * batch_size, dtype=torch.float32).cuda(configuration.cuda_device)\n",
    "    with torch.no_grad():\n",
    "        if windows:\n",
    "            det = model(images, target_res['img_scale'], target_res['img_size'])\n",
    "        if kaggle:\n",
    "            det = model(images, torch.tensor([1]*images.shape[0]).float().cuda())\n",
    "            \n",
    "        for i in range(images.shape[0]):\n",
    "            predictions = {'image_id':[],'conf': [],'boxes':[]}\n",
    "            # the first four columns are the boundaries of the boxes and the last 2 are the \n",
    "            score_threshold = score_threshold_orig\n",
    "            boxes = det[i].detach().cpu().numpy()[:,:4]\n",
    "            scores = det[i].detach().cpu().numpy()[:,4]\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            boxes = boxes[indexes]\n",
    "            scores = scores[indexes]\n",
    "            if boxes.shape[0] < 5:\n",
    "                score_threshold = 0.6\n",
    "#             else:\n",
    "#                 score_threshold = max((np.average(scores) - 0.2),score_threshold_orig)\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            boxes = boxes[indexes]\n",
    "            scores = scores[indexes]\n",
    "            boxes = boxes[indexes]\n",
    "            scores = scores[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            #predictions.append({'boxes': boxes[indexes],\n",
    "             #                 'scores': scores[indexes]})\n",
    "            predictions['boxes'] = boxes\n",
    "            predictions['conf'] = scores\n",
    "            predictions['image_id'] = image_ids[i]\n",
    "#             predictions['bad_conf'].append()\n",
    "            prediction_list.append(predictions)\n",
    "    return prediction_list\n",
    "#     with torch.no_grad():\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wbs(images, image_ids,models = model_list, score_threshold_orig = 0.22, skip_box_thr = 0.43):\n",
    "    prediction_list = []\n",
    "    for model in models:\n",
    "        prediction_list.append(make_predictions(images, image_ids, model = model, score_threshold_orig = 0.22))\n",
    "#         print(prediction_list)\n",
    "\n",
    "    predicts = []\n",
    "    prediction_df = pd.DataFrame(columns = ['image_id','conf','boxes'])\n",
    "    for prediction in prediction_list:\n",
    "        prediction_df = pd.concat([prediction_df,pd.DataFrame(prediction)], axis = 0)\n",
    "    nr_rows = prediction_df.shape[0]\n",
    "    pic_size = configuration.picture_dims[0]\n",
    "    prediction_df['normed_boxes'] = prediction_df['boxes'] / pic_size\n",
    "    for unique_id in prediction_df.image_id.unique():\n",
    "        boxes_list = []\n",
    "        scores_list = []\n",
    "        labels_list = []\n",
    "        small_df = prediction_df[prediction_df.image_id == unique_id].copy()\n",
    "        for i in range(small_df.shape[0]):\n",
    "            boxes_list.append(small_df.iloc[i].normed_boxes)\n",
    "            scores_list.append(small_df.iloc[i].conf)\n",
    "            labels_list.append([1]*small_df.iloc[i].conf.shape[0])\n",
    "        weights = [1]*len(models)\n",
    "        iou_thr = 0.55\n",
    "        max_len = 0\n",
    "        for array in scores_list:\n",
    "            if len(array) > max_len:\n",
    "                max_len = len(array)\n",
    "        if (max_len == 0):\n",
    "            predicts.append({'image_id' : unique_id, 'conf' : scores_list[0].astype(np.float32), 'boxes' : boxes_list[0].astype(np.float32)})\n",
    "            \n",
    "        else:\n",
    "            if len(models) == 1:\n",
    "                print('only one model')\n",
    "                boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list,\n",
    "                                                          weights = None, iou_thr = iou_thr,\n",
    "                                                         skip_box_thr = skip_box_thr)\n",
    "            else:\n",
    "                boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list,\n",
    "                                                          weights = weights, iou_thr = iou_thr,\n",
    "                                                         skip_box_thr = skip_box_thr)\n",
    "            prediction_dict = {'image_id' : unique_id, 'conf': scores.astype(np.float32), 'boxes': boxes.astype(np.float32)*pic_size}\n",
    "            predicts.append(prediction_dict)\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if configuration.visualize == True:\n",
    "    m = 0\n",
    "    new_imagelist = []\n",
    "    inf_folder = f'./inference'\n",
    "    if configuration.save & (not os.path.exists(inf_folder)):\n",
    "                os.makedirs(inf_folder)\n",
    "    for j, (images, image_ids) in enumerate(data_loader):\n",
    "        predictions = apply_wbs(images,image_ids, models = model_list ) \n",
    "#         predictions = make_predictions(images, image_ids)\n",
    "        df = pd.DataFrame(predictions)\n",
    "        for i in range(len(image_ids)):\n",
    "            ## plotting the inference\n",
    "            image_id = image_ids[i]\n",
    "            new_imagelist.append(image_id)\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(16, 16))\n",
    "            bx1 = df.iloc[i].boxes\n",
    "#             print(bx1)\n",
    "            conf1 = df.iloc[i].conf\n",
    "            sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "            k = 0\n",
    "            for box in bx1:\n",
    "                cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 1)\n",
    "                plt.text(box[2],box[1],f'{conf1[k]:.3f}', color = 'red', fontsize = 18)\n",
    "                k+=1\n",
    "            #Plotting the Ground Truth\n",
    "            if configuration.gt == True:\n",
    "                image1, target1, image_id1 = dataset1[m]\n",
    "                boxes1 = target1['boxes'].cpu().numpy().astype(np.int32)\n",
    "                for box1 in boxes1:\n",
    "                    cv2.rectangle(sample, (box1[1], box1[0]), (box1[3],  box1[2]), (0, 1, 0), 2)\n",
    "\n",
    "            #Showing and saving the picture\n",
    "            ax.set_axis_off()\n",
    "            ax.imshow(sample)\n",
    "            if windows & configuration.save:\n",
    "                plt.savefig(f'{inf_folder}/{image_ids[i]}.jpeg', transparent=True, bbox_inches = 'tight',\n",
    "                            facecolor = 'k',pad_inches = 0)\n",
    "            plt.show()\n",
    "            m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns = ['image_id', 'PredictionString'])\n",
    "cols = ['conf', 'px_min', 'py_min', 'px_max', 'py_max']\n",
    "\n",
    "for j, (images, image_ids) in enumerate(data_loader):\n",
    "    predictions = apply_wbs(images, image_ids, models = model_list) \n",
    "    df = pd.DataFrame(predictions)\n",
    "#     print('dataframe',df)\n",
    "    for i in range(len(image_ids)):\n",
    "        row = df.iloc[i]\n",
    "        filename = row.image_id\n",
    "        dataset = pd.DataFrame(columns = cols)\n",
    "        if len(row.conf) > 0:\n",
    "            for k in range(len(row.conf)):\n",
    "                row_df = pd.DataFrame([[row.conf[k],row.boxes[k][0],row.boxes[k][1],row.boxes[k][2],row.boxes[k][3]]], \n",
    "                                      columns = cols)\n",
    "                dataset = pd.concat([dataset, row_df], axis = 0, ignore_index = True)\n",
    "            dataset['width'] = configuration.picture_dims[0]\n",
    "            dataset['height'] = configuration.picture_dims[1]\n",
    "            scale_x = pic_scale[filename][1]/1024\n",
    "            scale_y = pic_scale[filename][0]/1024\n",
    "            dataset['px_box'] = dataset['px_max'] - dataset['px_min']\n",
    "            dataset['py_box'] = dataset['py_max'] - dataset['py_min']\n",
    "            dataset[['px_min', 'py_min', 'px_box', 'py_box']] = dataset[['px_min', 'py_min', 'px_box', 'py_box']].round().astype('int')\n",
    "            dataset.loc[dataset.width < dataset.px_min,'px_min'] = dataset.width -1\n",
    "            dataset.loc[dataset.height < dataset.py_min,'py_min'] = dataset.height -1\n",
    "            dataset.loc[1 > dataset.px_box,'px_box'] = 1\n",
    "            dataset.loc[1 > dataset.py_box,'py_box'] = 1\n",
    "            dataset.loc[0 > dataset.px_min,'px_min'] = 0\n",
    "            dataset.loc[0 > dataset.py_min,'py_min'] = 0\n",
    "            dataset.loc[dataset['px_min'] + dataset['px_box'] > dataset['width'], 'px_box'] = dataset['width'] - dataset['px_min'] - 1\n",
    "            dataset.loc[dataset['py_min'] + dataset['py_box'] > dataset['height'], 'py_box'] = dataset['height'] - dataset['py_min'] - 1\n",
    "            dataset[['px_min','px_box']] = (dataset[['px_min','px_box']] * scale_x).astype(int)\n",
    "            dataset[['py_min','py_box']] = (dataset[['py_min','py_box']] * scale_y).astype(int)\n",
    "            submission_cols = ['conf', 'px_min', 'py_min', 'px_box', 'py_box']\n",
    "            shape = dataset[submission_cols].shape\n",
    "            row1 = dataset[submission_cols].values.reshape(1,shape[0]*shape[1])\n",
    "            str_array = np.array2string(row1[0], formatter = {'float_kind': lambda x: '%.5f' % x}, max_line_width = 9e3)\n",
    "            str_array = re.compile(r'\\[([\\d\\s\\.]+)\\]').search(str_array).group(1)\n",
    "            vals = pd.DataFrame([[filename, str_array]], columns =['image_id', 'PredictionString'])\n",
    "        else:\n",
    "            vals = pd.DataFrame([[filename, '']], columns =['image_id', 'PredictionString'])\n",
    "        submission = pd.concat([submission,vals], axis = 0, ignore_index = True)\n",
    "        submission.PredictionString = submission.PredictionString.str.replace(r'(\\.0+)', '').str.replace('\\s+',' ')\n",
    "\n",
    "submission.to_csv('submission.csv', index = False, sep = ',')\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
